{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b5c42c3-25de-4280-bd5c-e3bc743235e3",
   "metadata": {},
   "source": [
    "# Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf214b8-1908-43a4-87bf-4ae0a9b11416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "from glob import glob\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import ntpath\n",
    "\n",
    "import numpy as np\n",
    "from imageio import imread\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, plot_confusion_matrix, matthews_corrcoef, classification_report,confusion_matrix, accuracy_score, balanced_accuracy_score, cohen_kappa_score, f1_score,  precision_score, recall_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from res_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ea51e-c85e-444c-a821-83d5f39d4eca",
   "metadata": {},
   "source": [
    "The file path of all the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2474874f-b7f0-4ba1-b067-b78d31ab6247",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = {\n",
    "        \"Ace_20\": \"/bigdata/haicu/venkat31/hemat/Acevedo/\", # Acevedo_20 Dataset\n",
    "        \"Mat_19\": \"/bigdata/haicu/venkat31/hemat/Matek/\", # Matek_19 Dataset\n",
    "        \"WBC1\": \"/bigdata/haicu/venkat31/hemat/WBC1/\" # WBC1 dataset\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd118ed-0893-4f87-983d-c8071d2146ef",
   "metadata": {},
   "source": [
    "All the labels in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c02d7cc4-8071-4e12-9e82-5c8787ab9f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map_all = {\n",
    "        'basophil': 0,\n",
    "        'eosinophil': 1,\n",
    "        'erythroblast': 2,\n",
    "        'myeloblast' : 3,\n",
    "        'promyelocyte': 4,\n",
    "        'myelocyte': 5,\n",
    "        'metamyelocyte': 6,\n",
    "        'neutrophil_banded': 7,\n",
    "        'neutrophil_segmented': 8,\n",
    "        'monocyte': 9,\n",
    "        'lymphocyte_typical': 10\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0201e68-bc8b-4096-bb14-13cdca1bdade",
   "metadata": {},
   "source": [
    "Read the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb56204-46f7-403c-93cb-853be290891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('./metadata.csv')\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977cc414-32ff-43f7-85ec-ba4b1bfe9450",
   "metadata": {},
   "source": [
    "A dataframe for the 3 different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8e02fc-4025-4cd2-94f7-6803835ab533",
   "metadata": {},
   "outputs": [],
   "source": [
    "ace_metadata=metadata.loc[metadata['dataset']=='Ace_20'].reset_index(drop = True)\n",
    "mat_metadata=metadata.loc[metadata['dataset']=='Mat_19'].reset_index(drop = True)\n",
    "wbc_metadata=metadata.loc[metadata['dataset']=='WBC1'].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1505fa7-5454-4a1a-9d03-27a8ebcc8684",
   "metadata": {},
   "source": [
    "## Data curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2849cf2-c82e-448f-bd10-2d377918fb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_Ace20=250\n",
    "crop_Mat19=345\n",
    "crop_WBC1=288\n",
    "\n",
    "dataset_image_size = {\n",
    "    \"Ace_20\":crop_Ace20,   #250,\n",
    "    \"Mat_19\":crop_Mat19,   #345, \n",
    "    \"WBC1\":crop_WBC1,   #288,  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcdf156-5c9c-471a-8644-fe8d937a52ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_metadata=metadata\n",
    "source_domains=['Ace_20', 'Mat_19']\n",
    "source_index = example_metadata.dataset.isin(source_domains)\n",
    "example_metadata = example_metadata.loc[source_index,:].copy().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d62d774-750e-4066-96c9-7b95971afab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fraction=0.2 #of the whole dataset\n",
    "val_fraction=0.125 #of 0.8 of the dataset (corresponds to 0.1 of the whole set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6825015-335d-46cc-b69b-5402aa5cc5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index, test_index, train_label, test_label = train_test_split(\n",
    "    example_metadata.index,\n",
    "    example_metadata.label + \"_\" + example_metadata.dataset,\n",
    "    test_size=test_fraction,\n",
    "    random_state=0, \n",
    "    shuffle=True,\n",
    "    stratify=example_metadata.label\n",
    "    )\n",
    "example_metadata.loc[test_index, 'set']='test'\n",
    "train_val_metadata=example_metadata.loc[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a49851a-899c-4342-8621-ae02d990fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index, val_index, train_label, val_label = train_test_split(\n",
    "    train_val_metadata.index,\n",
    "    train_val_metadata.label + \"_\" + train_val_metadata.dataset,\n",
    "    test_size=val_fraction,\n",
    "    random_state=0, \n",
    "    shuffle=True, \n",
    "    stratify=train_val_metadata.label\n",
    "    )\n",
    "example_metadata.loc[val_index, 'set']='val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f94d7af-9cc8-4392-be01-094228b54451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator(Dataset):\n",
    "\n",
    "    def __init__(self, \n",
    "                metadata, \n",
    "                reshape_size=64, \n",
    "                label_map=[],\n",
    "                dataset = [],\n",
    "                transform=None,\n",
    "                selected_channels = [0,1,2],\n",
    "                dataset_image_size=None):\n",
    "\n",
    "        self.metadata = metadata.copy().reset_index(drop = True)\n",
    "        self.label_map = label_map\n",
    "        self.transform = transform\n",
    "        self.selected_channels = selected_channels\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        ## get image and label\n",
    "        dataset =  self.metadata.loc[idx,\"dataset\"]\n",
    "        crop_size = dataset_image_size[dataset]\n",
    "        \n",
    "        h5_file_path = self.metadata.loc[idx,\"file\"]\n",
    "        image= imread(h5_file_path)[:,:,self.selected_channels]\n",
    "        image = image / 255.\n",
    "        h1 = (image.shape[0] - crop_size) /2\n",
    "        h1 = int(h1)\n",
    "        h2 = (image.shape[0] + crop_size) /2\n",
    "        h2 = int(h2)\n",
    "        \n",
    "        w1 = (image.shape[1] - crop_size) /2\n",
    "        w1 = int(w1)\n",
    "        w2 = (image.shape[1] + crop_size) /2\n",
    "        w2 = int(w2)\n",
    "        image = image[h1:h2,w1:w2, :]\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        label = self.metadata.loc[idx,\"label\"]\n",
    "        \n",
    "\n",
    "        # map numpy array to tensor\n",
    "        image = torch.from_numpy(copy.deepcopy(image)) \n",
    "        image = image.float()\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image) \n",
    "        \n",
    "        label = self.label_map[label]\n",
    "        label = torch.tensor(label).long()\n",
    "        return image.float(),  label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdeabac-29c8-4f71-a2b6-777db96439d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize=224 #image pixel size\n",
    "number_workers=0\n",
    "\n",
    "random_crop_scale=(0.8, 1.0)\n",
    "random_crop_ratio=(0.8, 1.2)\n",
    "\n",
    "mean=[0.485, 0.456, 0.406] #values from imagenet\n",
    "std=[0.229, 0.224, 0.225] #values from imagenet\n",
    "\n",
    "bs=32 #batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdab0f3-a436-4f6f-be41-1a39847e22d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization = torchvision.transforms.Normalize(mean,std)\n",
    "\n",
    "train_transform = transforms.Compose([ \n",
    "        normalization,\n",
    "        transforms.Resize(resize)\n",
    "        #transforms.RandomResizedCrop(resize, scale=random_crop_scale, ratio=random_crop_ratio),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip()\n",
    "        #transforms.RandomEqualize(p=0.8)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([ \n",
    "        normalization,\n",
    "        transforms.Resize(resize)])\n",
    "\n",
    "test_transform = transforms.Compose([ \n",
    "        normalization,\n",
    "        transforms.Resize(resize),\n",
    "        transforms.Resize(resize),\n",
    "        transforms.RandomApply(torch.nn.ModuleList([\n",
    "            transforms.ColorJitter(brightness=0.5, hue=0.1),\n",
    "            transforms.GaussianBlur(kernel_size=3, sigma=(0.5, 1))\n",
    "        ]), p=0.5),\n",
    "        transforms.RandomAdjustSharpness(sharpness_factor=0.2, p=0.5)])\n",
    "\n",
    "#dataset-creation\n",
    "\n",
    "train_dataset = DatasetGenerator(example_metadata.loc[train_index,:], \n",
    "                                 reshape_size=resize, \n",
    "                                 dataset = source_domains,\n",
    "                                 label_map=label_map_all, \n",
    "                                 transform = train_transform,\n",
    "                                 )\n",
    "val_dataset = DatasetGenerator(example_metadata.loc[val_index,:], \n",
    "                                 reshape_size=resize, \n",
    "                                 dataset = source_domains,\n",
    "                                 label_map=label_map_all, \n",
    "                                 transform = val_transform,\n",
    "                                 )\n",
    "\n",
    "test_dataset = DatasetGenerator(example_metadata.loc[test_index,:], \n",
    "                                 reshape_size=resize, \n",
    "                                 dataset = source_domains,\n",
    "                                 label_map=label_map_all, \n",
    "                                 transform = test_transform,\n",
    "                                 )\n",
    "train_loader = DataLoader(\n",
    "    test_dataset, batch_size=bs, shuffle=True, num_workers=number_workers)\n",
    "valid_loader = DataLoader(\n",
    "    val_dataset, batch_size=bs, shuffle=True, num_workers=number_workers)\n",
    "test_loader = DataLoader(\n",
    "    train_dataset, batch_size=bs, shuffle=False, num_workers=number_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497409c2-9e36-4a6a-ade4-16d923eb2efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10 # max number of epochs\n",
    "lr=1e-5 # learning rate\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181b7f7e-0ce6-4feb-9f4b-8f3453ae1891",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 11\n",
    "model = EqRes(n_rot=2, n_filter=32, n_class=num_classes)\n",
    "#model = torch.nn.DataParallel(model) \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8bc014-50fc-4da7-bad6-1c7af07a8cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-2, \n",
    "                                                steps_per_epoch=len(train_loader), \n",
    "                                               epochs=epochs+1, cycle_momentum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df881996-1d33-4fd6-b413-d6a8bf040bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path='theta2_filter32_fullaug' #path where model with best f1_macro should be stored\n",
    "\n",
    "#running variables\n",
    "epoch=0\n",
    "update_frequency=5 # number of batches before viewed acc and loss get updated\n",
    "counter=0 #counts batches\n",
    "f1_macro_best=0 #minimum f1_macro_score of the validation set for the first model to be saved\n",
    "loss_running=0\n",
    "acc_running=0\n",
    "val_batches=0\n",
    "\n",
    "y_pred=torch.tensor([], dtype=int)\n",
    "y_true=torch.tensor([], dtype=int)\n",
    "y_pred=y_pred.to(device)\n",
    "y_true=y_true.to(device)\n",
    "\n",
    "\n",
    "#Training\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    #training\n",
    "    model.train()\n",
    "    \n",
    "    with tqdm(train_loader) as tepoch:   \n",
    "        for i, data in enumerate(tepoch):\n",
    "            tepoch.set_description(f\"Epoch {epoch+1}\")\n",
    "            counter+=1\n",
    "\n",
    "            x, y = data\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            logits = torch.softmax(out.detach(), dim=1)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            acc = accuracy_score(y.cpu(), predictions.cpu())\n",
    "            \n",
    "            if counter >= update_frequency:\n",
    "                tepoch.set_postfix(loss=loss.item(), accuracy=acc.item())\n",
    "                counter=0\n",
    "                \n",
    "    #validation       \n",
    "    model.eval()\n",
    "    with tqdm(valid_loader) as vepoch: \n",
    "        for i, data in enumerate(vepoch):\n",
    "            vepoch.set_description(f\"Validation {epoch+1}\")\n",
    "\n",
    "            x, y = data\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            \n",
    "            logits = torch.softmax(out.detach(), dim=1)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            y_pred=torch.cat((y_pred, predictions), 0)\n",
    "            y_true=torch.cat((y_true, y), 0)\n",
    "            \n",
    "            acc = accuracy_score(y_true.cpu(), y_pred.cpu())\n",
    "            \n",
    "            loss_running+=(loss.item()*len(y))\n",
    "            acc_running+=(acc.item()*len(y))\n",
    "            val_batches+=len(y)\n",
    "            loss_mean=loss_running/val_batches\n",
    "            acc_mean=acc_running/val_batches\n",
    "            \n",
    "            vepoch.set_postfix(loss=loss_mean, accuracy=acc_mean)\n",
    "            \n",
    "        f1_micro=f1_score(y_true.cpu(), y_pred.cpu(), average='micro')\n",
    "        f1_macro=f1_score(y_true.cpu(), y_pred.cpu(), average='macro')\n",
    "        print(f'f1_micro: {f1_micro}, f1_macro: {f1_macro}')  \n",
    "        if f1_macro > f1_macro_best:\n",
    "            f1_macro_best=f1_macro\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print('model saved')\n",
    "        \n",
    "        #reseting running variables\n",
    "        loss_running=0\n",
    "        acc_running=0\n",
    "        val_batches=0\n",
    "            \n",
    "        y_pred=torch.tensor([], dtype=int)\n",
    "        y_true=torch.tensor([], dtype=int)\n",
    "        y_pred=y_pred.to(device)\n",
    "        y_true=y_true.to(device)\n",
    "            \n",
    "        \n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716d3055-98c4-431f-89ec-bb81cb767617",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('theta2_filter32_fullaug_new'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e85981e-ee5b-4771-b79a-4e9329e1b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_test=example_metadata.loc[test_index,:]\n",
    "ace_metadata_test=metadata_test.loc[metadata_test['dataset']=='Ace_20'].reset_index(drop = True)\n",
    "mat_metadata_test=metadata_test.loc[metadata_test['dataset']=='Mat_19'].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a119c09e-ec24-45c8-891c-9f6433971a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(metadata=metadata_test, \n",
    "               source_domains=['Ace_20', 'Mat_19'], label_map=label_map_all):\n",
    "\n",
    "\n",
    "\n",
    "    pred_dataset = DatasetGenerator(metadata, \n",
    "                                 reshape_size=resize, \n",
    "                                 dataset = source_domains,\n",
    "                                 label_map=label_map, \n",
    "                                 transform = test_transform,\n",
    "                                 )\n",
    "    \n",
    "    pred_loader = DataLoader(pred_dataset, \n",
    "                             batch_size=1, \n",
    "                             shuffle=False, \n",
    "                             num_workers=6\n",
    "                            )\n",
    "    n=len(pred_loader)\n",
    "    model.eval()\n",
    "    preds=torch.tensor([], dtype=int)\n",
    "    preds=preds.to(device)\n",
    "    prediction=torch.tensor([])\n",
    "    prediction=prediction.to(device)\n",
    "    for i, data in enumerate(pred_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        x, y = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        logits = torch.softmax(out.detach(), dim=1)\n",
    "        prediction = torch.cat((prediction, logits), 0)\n",
    "        predic = logits.argmax(dim=1)\n",
    "        preds=torch.cat((preds, predic), 0)\n",
    "\n",
    "    preds=preds.cpu()\n",
    "    preds=preds.detach().numpy()\n",
    "    np.save('preds', preds)\n",
    "    y_pred = [label_map_reverse[p] for p in  preds]\n",
    "    y_true=metadata['label']\n",
    "    return y_true, y_pred, preds\n",
    "\n",
    "def classification_complete_report(y_true, y_pred ,labels = None  ): \n",
    "    print(classification_report(y_true, y_pred, labels = None))\n",
    "    print(15*\"----\")\n",
    "    print(\"matthews correlation coeff: %.4f\" % (matthews_corrcoef(y_true, y_pred)) )\n",
    "    print(\"Cohen Kappa score: %.4f\" % (cohen_kappa_score(y_true, y_pred)) )\n",
    "    print(\"Accuracy: %.4f & balanced Accuracy: %.4f\" % (accuracy_score(y_true, y_pred), balanced_accuracy_score(y_true, y_pred)) )\n",
    "    #print(\"macro F1 score: %.4f & micro F1 score: %.4f\" % (f1_score(y_true, y_pred, average = \"macro\"), f1_score(y_true, y_pred, average = \"micro\")) )\n",
    "    print(\"macro Precision score: %.4f & micro Precision score: %.4f\" % (precision_score(y_true, y_pred, average = \"macro\"), precision_score(y_true, y_pred, average = \"micro\")) )\n",
    "    print(\"macro Recall score: %.4f & micro Recall score: %.4f\" % (recall_score(y_true, y_pred, average = \"macro\"), recall_score(y_true, y_pred, average = \"micro\")) )\n",
    "    print(labels)\n",
    "    cm = confusion_matrix(y_true, y_pred,labels= labels, normalize='true')\n",
    "    fig, ax = plt.subplots(figsize=(10, 10)) #plot size\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical', ax=ax, include_values=False, colorbar=False)\n",
    "    \n",
    "    plt.show()\n",
    "    print(15*\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc579f62-0fcf-4c24-83a5-25a1d7f5cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred, preds=prediction(metadata= ace_metadata_test, source_domains=['Ace_20'])\n",
    "classification_complete_report(y_true, y_pred, labels=label_list_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a2d7d9-985d-4ebc-941b-0a573b73fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred, preds=prediction(metadata= mat_metadata_test, source_domains=['Mat_19'])\n",
    "classification_complete_report(y_true, y_pred, labels=label_list_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea7b874-f895-49c3-b057-9b41ce157fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred, preds=prediction(metadata=wbc_metadata, source_domains=['WBC1'], label_map=label_map_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b88b1c3-3cba-4bf7-bb51-36c2ed079d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdata=wbc_metadata.drop(columns=['file', 'label', 'dataset', 'set', 'mean1', 'mean2', 'mean3'])\n",
    "outputdata['Label']=y_pred\n",
    "outputdata['LabelID']=preds\n",
    "'''\n",
    "for i in range(len(y_pred)):\n",
    "    outputdata['LabelID'].loc[i]=y_pred[i]\n",
    "    outputdata['Label'].loc[i]=label_map_reverse[y_pred[i]]\n",
    "'''\n",
    "outputdata.to_csv('submission_new2_aug.csv')\n",
    "print(outputdata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdl",
   "language": "python",
   "name": "gdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
